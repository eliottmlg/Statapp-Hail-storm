{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import statistics \n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting st-dbscan\n",
      "  Using cached st_dbscan-0.2.2.tar.gz (414 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages (from st-dbscan) (1.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages (from st-dbscan) (1.10.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.0-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 165.5 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 132.5 kB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, st-dbscan\n",
      "  Running setup.py install for st-dbscan: started\n",
      "  Running setup.py install for st-dbscan: finished with status 'done'\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.0 st-dbscan-0.2.2 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "  DEPRECATION: st-dbscan is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\t480\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'st_dbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install st-dbscan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mst_dbscan\u001b[39;00m \u001b[39mimport\u001b[39;00m ST_DBSCAN\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install geopy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgeopy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'st_dbscan'"
     ]
    }
   ],
   "source": [
    "!pip install st-dbscan\n",
    "from st_dbscan import ST_DBSCAN\n",
    "!pip install geopy\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix = pd.read_csv(\"C:/Users/t480/Desktop/University courses/ENSAE 2A/STATAPP/Statapp-Hail-storm/bdd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_timestamps(df):\n",
    "    df['TIME_EVENT'] = pd.to_datetime(df['TIME_EVENT'],format = '%Y-%m-%d %H:%M:%S')\n",
    "    df['TIME_EVENT'] = df.TIME_EVENT.values.astype(np.float64) # in nanoseconds\n",
    "    df.TIME_EVENT = df.TIME_EVENT / (10 ** 9) # in seconds \n",
    "    df.TIME_EVENT = df.TIME_EVENT / 3600 # in hours\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_ready(df, country, year_since):\n",
    "    df = convert_time_to_timestamps(df)\n",
    "    df = df[df['COUNTRY'] == country]\n",
    "    df = df[df['YEAR'] >= year_since]\n",
    "    df = df[['TIME_EVENT', 'LATITUDE', 'LONGITUDE']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscan_retrieve_classes(df, spatial_th, time_th, min_sample):\n",
    "    model = ST_DBSCAN(eps1=spatial_th, eps2=time_th, min_samples=min_sample)\n",
    "    fit = model.fit(df) \n",
    "    labels = pd.DataFrame(model.labels)\n",
    "    df['LABELS'] = labels\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the timing of reports with an intensity map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_biggest(df, n_biggest):\n",
    "\n",
    "    nb_reports = pd.DataFrame(df['LABELS'].value_counts()).reset_index()\n",
    "    nb_reports = nb_reports[nb_reports['index'] != -1].sort_values('LABELS', ascending=False).reset_index()\n",
    "    labels_biggest_storms = list(nb_reports.iloc[0:3,1])\n",
    "    df_biggest = df.loc[df.LABELS.isin(labels_biggest_storms)]\n",
    "\n",
    "    # plots the n_biggest storms \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(n_biggest):\n",
    "        a = df_biggest.LONGITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        b = df_biggest.LATITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        lab = df_biggest.loc[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        plt.scatter(a, b, c='C{}'.format(i), label='{} most reported storm'.format(i+1))\n",
    "    plt.ylabel('latitude')\n",
    "    plt.xlabel('longitude')\n",
    "    plt.legend()\n",
    "    plt.title(\"The {} biggest storms in terms of report counts\".format(len(labels_biggest_storms)))\n",
    "    plt.show()\n",
    "\n",
    "    # plot timing of storms in latlon\n",
    "    for i in range(n_biggest):\n",
    "        x = df_biggest.LONGITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        y = df_biggest.LATITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        z = df_biggest.TIME_EVENT[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        z = z - z.iloc[-1]\n",
    "        x = x.array\n",
    "        y = y.array\n",
    "        z = z.array\n",
    "        axis = plt.subplot(111, title=\"Storm with {} reports\".format(len(x)))\n",
    "        sc = axis.scatter(x, y, c=z, marker=\".\")\n",
    "        plt.colorbar(sc, label=\"duration of the storm (in hours)\")\n",
    "        plt.show()\n",
    "        # distance between first and last reports\n",
    "        df_distance = df_biggest[df_biggest.LABELS==labels_biggest_storms[i]].sort_values('TIME_EVENT')\n",
    "        df_distance = df_distance.reset_index(drop=True)\n",
    "        latbegin = df_distance.LATITUDE[0]\n",
    "        latend = df_distance.LATITUDE[len(df_distance.LATITUDE)-1]\n",
    "        lonbegin = df_distance.LONGITUDE[0]\n",
    "        lonend = df_distance.LONGITUDE[len(df_distance.LONGITUDE)-1]\n",
    "        coords_1 = (latbegin, lonbegin)\n",
    "        coords_2 = (latend, lonend)\n",
    "        dist = geopy.distance.geodesic(coords_1, coords_2).km\n",
    "        print(\"The storm has length\",dist,\"km between first report and last report\")\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIME_EVENT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462946.250000</td>\n",
       "      <td>45.567</td>\n",
       "      <td>5.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462945.866667</td>\n",
       "      <td>45.596</td>\n",
       "      <td>5.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>462928.500000</td>\n",
       "      <td>49.583</td>\n",
       "      <td>1.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>462928.416667</td>\n",
       "      <td>49.533</td>\n",
       "      <td>1.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462928.383333</td>\n",
       "      <td>49.521</td>\n",
       "      <td>1.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>458531.666667</td>\n",
       "      <td>43.279</td>\n",
       "      <td>2.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>458514.500000</td>\n",
       "      <td>44.600</td>\n",
       "      <td>-0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>458174.283333</td>\n",
       "      <td>48.669</td>\n",
       "      <td>5.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>458174.166667</td>\n",
       "      <td>48.636</td>\n",
       "      <td>4.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>457481.000000</td>\n",
       "      <td>44.750</td>\n",
       "      <td>-0.533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TIME_EVENT  LATITUDE  LONGITUDE\n",
       "0     462946.250000    45.567      5.933\n",
       "1     462945.866667    45.596      5.875\n",
       "2     462928.500000    49.583      1.358\n",
       "3     462928.416667    49.533      1.267\n",
       "4     462928.383333    49.521      1.251\n",
       "...             ...       ...        ...\n",
       "1975  458531.666667    43.279      2.460\n",
       "1976  458514.500000    44.600     -0.933\n",
       "1977  458174.283333    48.669      5.114\n",
       "1978  458174.166667    48.636      4.952\n",
       "1979  457481.000000    44.750     -0.533\n",
       "\n",
       "[1980 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_fix.copy()\n",
    "country = 'FR'\n",
    "year_since = 2022\n",
    "df = getting_ready(df, country, year_since)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ST_DBSCAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[39m=\u001b[39m run_dbscan_retrieve_classes(df, \u001b[39m0.2\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m visualise_biggest(df1,\u001b[39m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m, in \u001b[0;36mrun_dbscan_retrieve_classes\u001b[1;34m(df, spatial_th, time_th, min_sample)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_dbscan_retrieve_classes\u001b[39m(df, spatial_th, time_th, min_sample):\n\u001b[1;32m----> 2\u001b[0m     model \u001b[39m=\u001b[39m ST_DBSCAN(eps1\u001b[39m=\u001b[39mspatial_th, eps2\u001b[39m=\u001b[39mtime_th, min_samples\u001b[39m=\u001b[39mmin_sample)\n\u001b[0;32m      3\u001b[0m     fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(df) \n\u001b[0;32m      4\u001b[0m     labels \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(model\u001b[39m.\u001b[39mlabels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ST_DBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = run_dbscan_retrieve_classes(df, 0.2, 2, 2)\n",
    "visualise_biggest(df1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varying spatial threshold\n",
    "#for i in np.arange(0.01,0.3,0.03):\n",
    "#    visualise_biggest(run_dbscan_retrieve_classes(df, i, 2, 2),3)\n",
    "#    print(\"Spatial threshold is\",i)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donnees demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t480\\AppData\\Local\\Temp\\ipykernel_6568\\3418599384.py:1: DtypeWarning: Columns (18,22,23,24,50,51,77,78,79,85,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  database = pd.read_csv(\"C:/Users/t480/Desktop/University courses/ENSAE 2A/STATAPP/Statapp-Hail-storm/eswd_reports_europe_1970_2022.csv\")\n"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv(\"C:/Users/t480/Desktop/University courses/ENSAE 2A/STATAPP/Statapp-Hail-storm/eswd_reports_europe_1970_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name)\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_demog \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/t480/Desktop/University courses/ENSAE 2A/STATAPP/Statapp-Hail-storm/data demog.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1695\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m engine\n\u001b[0;32m   1693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_options \u001b[39m=\u001b[39m storage_options\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engines[engine](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_io, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:556\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39m@doc\u001b[39m(storage_options\u001b[39m=\u001b[39m_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    544\u001b[0m     filepath_or_buffer: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m    545\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    546\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[39m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39m    {storage_options}\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39;49m\u001b[39mopenpyxl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    557\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[39m=\u001b[39mstorage_options)\n",
      "File \u001b[1;32mc:\\Users\\t480\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:144\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    145\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "data_demog = pd.read_excel(\"C:/Users/t480/Desktop/University courses/ENSAE 2A/STATAPP/Statapp-Hail-storm/data demog.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TIME_EVENT</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45.567</td>\n",
       "      <td>5.933</td>\n",
       "      <td>2022-10-24 10:15:00</td>\n",
       "      <td>FR</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>45.596</td>\n",
       "      <td>5.875</td>\n",
       "      <td>2022-10-24 09:52:00</td>\n",
       "      <td>FR</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>50.967</td>\n",
       "      <td>2.667</td>\n",
       "      <td>2022-10-23 17:00:00</td>\n",
       "      <td>BE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>51.000</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2022-10-23 17:00:00</td>\n",
       "      <td>BE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49.583</td>\n",
       "      <td>1.358</td>\n",
       "      <td>2022-10-23 16:30:00</td>\n",
       "      <td>FR</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43607</th>\n",
       "      <td>43607</td>\n",
       "      <td>48.750</td>\n",
       "      <td>7.100</td>\n",
       "      <td>2007-02-11 15:10:00</td>\n",
       "      <td>FR</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43608</th>\n",
       "      <td>43608</td>\n",
       "      <td>52.150</td>\n",
       "      <td>14.650</td>\n",
       "      <td>2007-01-18 19:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43609</th>\n",
       "      <td>43609</td>\n",
       "      <td>52.083</td>\n",
       "      <td>10.333</td>\n",
       "      <td>2007-01-18 17:10:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43610</th>\n",
       "      <td>43610</td>\n",
       "      <td>43.600</td>\n",
       "      <td>39.730</td>\n",
       "      <td>2007-01-16 02:00:00</td>\n",
       "      <td>RU</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43611</th>\n",
       "      <td>43611</td>\n",
       "      <td>53.367</td>\n",
       "      <td>9.717</td>\n",
       "      <td>2007-01-10 18:05:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43612 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  LATITUDE  LONGITUDE           TIME_EVENT COUNTRY  YEAR\n",
       "0               0    45.567      5.933  2022-10-24 10:15:00      FR  2022\n",
       "1               1    45.596      5.875  2022-10-24 09:52:00      FR  2022\n",
       "2               2    50.967      2.667  2022-10-23 17:00:00      BE  2022\n",
       "3               3    51.000      2.700  2022-10-23 17:00:00      BE  2022\n",
       "4               4    49.583      1.358  2022-10-23 16:30:00      FR  2022\n",
       "...           ...       ...        ...                  ...     ...   ...\n",
       "43607       43607    48.750      7.100  2007-02-11 15:10:00      FR  2007\n",
       "43608       43608    52.150     14.650  2007-01-18 19:00:00      DE  2007\n",
       "43609       43609    52.083     10.333  2007-01-18 17:10:00      DE  2007\n",
       "43610       43610    43.600     39.730  2007-01-16 02:00:00      RU  2007\n",
       "43611       43611    53.367      9.717  2007-01-10 18:05:00      DE  2007\n",
       "\n",
       "[43612 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'QC_LEVEL', 'INFO_SOURCE', 'CONTACT', 'EMAIL', 'ORGANISATION',\n",
       "       'ORGANISATION_ID', 'NO_REVISION', 'PERSON_REVISION', 'TIME_EVENT',\n",
       "       'TIME_CREATION', 'TIME_LAST_REVISION', 'TIME_ACCURACY', 'COUNTRY',\n",
       "       'STATE', 'PLACE', 'PLACE_LOCAL_LANGUAGE', 'DETAILED_LOCATION',\n",
       "       'NEAREST_CITY', 'LATITUDE', 'LONGITUDE', 'PLACE_ACCURACY', 'OROGRAPHY',\n",
       "       'SURFACE_INITIAL_LOCATION', 'SURFACE_CROSSED', 'TYPE_EVENT',\n",
       "       'NO_OBJECTS', 'MAX_HAIL_DIAMETER', 'MAX_HAILSTONE_WEIGHT',\n",
       "       'AVERAGE_HAIL_DIAMETER', 'THICKNESS_HAIL_LAYER', 'HAILSTONE', 'F_SCALE',\n",
       "       'T_SCALE', 'RATING_BASIS', 'WIND_SPEED', 'TEN_MIN_WIND_SPEED',\n",
       "       'FUNNEL_SIGHTED', 'SUCTION_VORTICES', 'PRECIPITATION_AMOUNT',\n",
       "       'SNOW_FALL_AMOUNT', 'PEAK_PRECIP_AMOUNT', 'PEAK_SNOW_FALL_AMOUNT',\n",
       "       'PEAK_PRECIP_PERIOD', 'MAX_6_HOUR_PRECIP', 'MAX_6_HOUR_SNOW_FALL',\n",
       "       'MAX_12_HOUR_PRECIP', 'MAX_12_HOUR_SNOW_FALL', 'MAX_24_HOUR_PRECIP',\n",
       "       'MAX_24_HOUR_SNOW_FALL', 'CONVECTIVE', 'TOTAL_DURATION', 'TYPE_PRECIP',\n",
       "       'SIZE_ACCOMPANYING_HAIL', 'POSSIBILITIES', 'PATH_LENGTH',\n",
       "       'MEAN_PATH_WIDTH', 'MAX_PATH_WIDTH', 'MAX_VERTICAL_DEVELOP',\n",
       "       'DIRECTION_MOVEMENT', 'SNOW_HAZARDS', 'MEAN_HEIGHT_SNOW_CORNICES',\n",
       "       'MAX_HEIGHT_SNOW_CORNICES', 'ICE_HAZARDS', 'THICKNESS_ICE_COVER',\n",
       "       'THICKNESS_RIME_COVER', 'AVALANCHE_TYPE', 'AVALANCHE_FLOW_TYPE',\n",
       "       'SNOW_MASS_TYPE', 'AVALANCHE_SIZE', 'AVALANCHE_TRIGGER',\n",
       "       'ELEVATION_START', 'ELEVATION_DIFFERENCE', 'LIGHTNING_DAMAGE_TO',\n",
       "       'PEAK_CURRENT', 'POLARITY', 'EXCEPT_ELEC_PHENOM', 'PROPERTY_DAMAGE',\n",
       "       'CROP_FOREST_DAMAGE', 'TOTAL_DAMAGE', 'NO_INJURED', 'NO_KILLED',\n",
       "       'EVENT_DESCRIPTION', 'PATH_START_LATITUDE', 'PATH_START_LONGITUDE',\n",
       "       'PATH_START_DATETIME', 'PATH_END_LATITUDE', 'PATH_END_LONGITUDE',\n",
       "       'PATH_END_DATETIME', 'EXT_URL', 'REFERENCE', 'IMPACTS', 'CREATOR_ID',\n",
       "       'REVISOR_ID', 'LINK_ORG', 'LINK_ID', 'DELETED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>PLACE</th>\n",
       "      <th>DETAILED_LOCATION</th>\n",
       "      <th>NEAREST_CITY</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Région Rhône-Alpes</td>\n",
       "      <td>Chambéry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Région Rhône-Alpes</td>\n",
       "      <td>La Motte-Servolex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Région Haute-Normandie</td>\n",
       "      <td>Buchy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Région Haute-Normandie</td>\n",
       "      <td>Bierville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Région Haute-Normandie</td>\n",
       "      <td>Morgny-la-Pommeraye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48369</th>\n",
       "      <td>Région Poitou-Charentes</td>\n",
       "      <td>Saint-Hilaire-la-Palud</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48375</th>\n",
       "      <td>Région Centre</td>\n",
       "      <td>Montargis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48376</th>\n",
       "      <td>Région Centre</td>\n",
       "      <td>Villemurlin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48377</th>\n",
       "      <td>Région Centre</td>\n",
       "      <td>Villequiers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48380</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mont-de-Marsan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4008 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         STATE                   PLACE DETAILED_LOCATION  \\\n",
       "0           Région Rhône-Alpes                Chambéry               NaN   \n",
       "1           Région Rhône-Alpes       La Motte-Servolex               NaN   \n",
       "4       Région Haute-Normandie                   Buchy               NaN   \n",
       "5       Région Haute-Normandie               Bierville               NaN   \n",
       "6       Région Haute-Normandie     Morgny-la-Pommeraye               NaN   \n",
       "...                        ...                     ...               ...   \n",
       "48369  Région Poitou-Charentes  Saint-Hilaire-la-Palud               NaN   \n",
       "48375            Région Centre               Montargis               NaN   \n",
       "48376            Région Centre             Villemurlin               NaN   \n",
       "48377            Région Centre             Villequiers               NaN   \n",
       "48380                      NaN          Mont-de-Marsan               NaN   \n",
       "\n",
       "      NEAREST_CITY COUNTRY  \n",
       "0              NaN      FR  \n",
       "1              NaN      FR  \n",
       "4              NaN      FR  \n",
       "5              NaN      FR  \n",
       "6              NaN      FR  \n",
       "...            ...     ...  \n",
       "48369          NaN      FR  \n",
       "48375          NaN      FR  \n",
       "48376          NaN      FR  \n",
       "48377          NaN      FR  \n",
       "48380          NaN      FR  \n",
       "\n",
       "[4008 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.columns\n",
    "geo = database[[\"STATE\", 'PLACE', 'DETAILED_LOCATION','NEAREST_CITY', 'COUNTRY']]\n",
    "geo = geo[geo['COUNTRY'] == 'FR']\n",
    "geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_demog = data_demog.rename(columns = {'Insee - Statistiques locales':'POSTCODE', 'Unnamed: 1':'PLACE', 'Unnamed: 2':'POPULATION'})\n",
    "data_demog = data_demog.iloc[3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot merge our data with data from insee since there is a lot of cities that will not have a track.\n",
    "# we need the latitude and longitude of each cities in the insee data\n",
    "# need to study the shape of storms, if it is a rectangle, that is rather straight, \n",
    "# with some depending thickness, then we can start to infer movements\n",
    "# say we have two clusters and they are in a rectangle, but in the middle there is no track\n",
    "# then it may be the same storm. \n",
    "# we can compute an average reporting rate, then multiply with the population in the cities where no tracks appear\n",
    "# if this expected number of reports is lower than a threshold, it may be the two clusters \n",
    "# represent only one storm\n",
    "# we also need to inspect a few KM ahead and after the cluster, whether or not another cluster may be related \n",
    "# this is to observe formation and death of each storm, that may be less intense zones but still \n",
    "# important to study : time to reach peak activity for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to investigate whether we miss-detect the area of the storm because some regions are not populated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute rate of report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea of algorithm \n",
    "# we run dbscan with restrictive parameters,\n",
    "# then for each cluster, we take the cities in the rectangle and we compute their expected nb of reports \n",
    "# if the figure is above the threshold we keep it as is\n",
    "# but if it is below, we run dbscan with less restrictive parameters on a subset of obs\n",
    "# in particular we delete all observations not in the trajectory \n",
    "# (the rectangle, with an arbitrarily big length and width) and that are not timely coherent \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATISTICS ON STORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_biggest(df, n_biggest):\n",
    "\n",
    "    nb_reports = pd.DataFrame(df['LABELS'].value_counts()).reset_index()\n",
    "    nb_reports = nb_reports[nb_reports['index'] != -1].sort_values('LABELS', ascending=False).reset_index()\n",
    "    labels_biggest_storms = list(nb_reports.iloc[0:3,1])\n",
    "    df_biggest = df.loc[df.LABELS.isin(labels_biggest_storms)]\n",
    "\n",
    "    # plots the n_biggest storms \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(n_biggest):\n",
    "        a = df_biggest.LONGITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        b = df_biggest.LATITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        lab = df_biggest.loc[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        plt.scatter(a, b, c='C{}'.format(i), label='{} most reported storm'.format(i+1))\n",
    "    plt.ylabel('latitude')\n",
    "    plt.xlabel('longitude')\n",
    "    plt.legend()\n",
    "    plt.title(\"The {} biggest storms in terms of report counts\".format(len(labels_biggest_storms)))\n",
    "    plt.show()\n",
    "\n",
    "    # plot timing of storms in latlon\n",
    "    for i in range(n_biggest):\n",
    "        x = df_biggest.LONGITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        y = df_biggest.LATITUDE[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        z = df_biggest.TIME_EVENT[df_biggest.LABELS == labels_biggest_storms[i]]\n",
    "        z = z - z.iloc[-1]\n",
    "        x = x.array\n",
    "        y = y.array\n",
    "        z = z.array\n",
    "        axis = plt.subplot(111, title=\"Storm with {} reports\".format(len(x)))\n",
    "        sc = axis.scatter(x, y, c=z, marker=\".\")\n",
    "        plt.colorbar(sc, label=\"duration of the storm (in hours)\")\n",
    "        plt.show()\n",
    "        # distance between first and last reports\n",
    "        df_distance = df_biggest[df_biggest.LABELS==labels_biggest_storms[i]].sort_values('TIME_EVENT')\n",
    "        df_distance = df_distance.reset_index(drop=True)\n",
    "        latbegin = df_distance.LATITUDE[0]\n",
    "        latend = df_distance.LATITUDE[len(df_distance.LATITUDE)-1]\n",
    "        lonbegin = df_distance.LONGITUDE[0]\n",
    "        lonend = df_distance.LONGITUDE[len(df_distance.LONGITUDE)-1]\n",
    "        coords_1 = (latbegin, lonbegin)\n",
    "        coords_2 = (latend, lonend)\n",
    "        dist = geopy.distance.geodesic(coords_1, coords_2).km\n",
    "        print(\"The storm has length\",dist,\"km between first report and last report\")\n",
    "\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f19d07077eb1cdac0b0b0af8eb107740074d3c18ca2a297d764b781592c2b226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
